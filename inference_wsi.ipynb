{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, filters, transform \n",
    "import tifffile as tiff\n",
    "import albumentations as A\n",
    " \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    DEVICE = \"cuda\"\n",
    "    FOLDS = 5\n",
    "    LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HubDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,reader, coords_list,):\n",
    "        self.reader = reader\n",
    "        self.coords_list = coords_list\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.coords_list)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        \n",
    "        coords = self.coords_list[item]\n",
    "        image = self.reader.get_tiles(coords[0], coords[1])\n",
    "\n",
    "        image = image.astype(np.uint8)/255    \n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"image\": torch.tensor(image, dtype=torch.float),\n",
    "            \"coords\": coords,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()    \n",
    "def infer(model,valid_loader,device):\n",
    "    model.eval()\n",
    "    final_coords = []\n",
    "    masks = []\n",
    "    for data in valid_loader:\n",
    "        inputs = data['image']\n",
    "        \n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "\n",
    "        output = model(inputs,)  \n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        output = output[:,0,:,:].detach().cpu().numpy()  ## B, H, W\n",
    "\n",
    "        ## postprocess\n",
    "        for idx in range(output.shape[0]):\n",
    "            threshold = filters.threshold_mean(output[idx]) ##  isodata, otsu, li, mean, yen, minimum\n",
    "            mask = output[idx] > threshold\n",
    "            mask = mask.astype(np.int8)*255\n",
    "            masks.append(mask)\n",
    "            \n",
    "    return masks  #, final_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('CoAT/')\n",
    "\n",
    "from coat import *\n",
    "from daformer import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class MixUpSample(nn.Module):\n",
    "    def __init__(self, scale_factor=4):\n",
    "        super().__init__()\n",
    "        self.mixing = nn.Parameter(torch.tensor(0.5))\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mixing * F.interpolate(\n",
    "            x, scale_factor=self.scale_factor, mode=\"bilinear\", align_corners=False\n",
    "        ) + (1 - self.mixing) * F.interpolate(\n",
    "            x, scale_factor=self.scale_factor, mode=\"nearest\"\n",
    "        )\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 encoder=coat_lite_medium,\n",
    "                 decoder=daformer_conv3x3,\n",
    "                 encoder_cfg={},\n",
    "                 decoder_cfg={},\n",
    "                 ):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        decoder_dim = decoder_cfg.get('decoder_dim', 320)\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.rgb = RGB()\n",
    "\n",
    "        encoder_dim = self.encoder.embed_dims\n",
    "        # [64, 128, 320, 512]\n",
    "\n",
    "        self.decoder = decoder(\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim,\n",
    "        )\n",
    "#         self.logit = nn.Sequential(\n",
    "#             nn.Conv2d(decoder_dim, 1, kernel_size=1),\n",
    "#             nn.Upsample(scale_factor = 4, mode='bilinear', align_corners=False),\n",
    "#         )\n",
    "        self.logit = nn.Conv2d(decoder_dim, 1, kernel_size=1)\n",
    "        self.mixup = MixUpSample()\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.rgb(x)\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "        encoder = self.encoder(x)\n",
    "\n",
    "        last, decoder = self.decoder(encoder)\n",
    "        logits = self.logit(last)\n",
    "        \n",
    "        upsampled_logits = self.mixup(logits)\n",
    "        \n",
    "        return upsampled_logits\n",
    "    \n",
    "\n",
    "### encoder\n",
    "class coat_parallel_small_plus1 (CoaT):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(coat_parallel_small_plus1, self).__init__(\n",
    "            patch_size=4,\n",
    "            embed_dims=[152, 320, 320, 320, 320],\n",
    "            serial_depths=[2, 2, 2, 2, 2],\n",
    "            parallel_depth=6,\n",
    "            num_heads=8,\n",
    "            mlp_ratios=[4, 4, 4, 4, 4],\n",
    "            pretrain ='coat_small_7479cf9b.pth',\n",
    "            **kwargs)\n",
    "\n",
    "\n",
    "def HubmapModel():\n",
    "    encoder = coat_lite_medium()\n",
    "    checkpoint = 'coat_lite_medium_384x384_f9129688.pth'\n",
    "    checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "    state_dict = checkpoint['model']\n",
    "    encoder.load_state_dict(state_dict,strict=False)\n",
    "    \n",
    "    net = Net(encoder=encoder).cuda()\n",
    "    \n",
    "    return net\n",
    "\n",
    "model_paths = [\n",
    "              \"weights/model-0.pth\",\n",
    "              \"weights/model-1.pth\",\n",
    "              \"weights/model-2.pth\",\n",
    "              \"weights/model-3.pth\",\n",
    "              \"weights/model-4.pth\", \n",
    "              ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSI inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSI = \"/mnt/prj001/Rama_Downloaded/hamarepository_data/69026_H&E.ndpi\"\n",
    "batch_size = 4\n",
    "import matplotlib.pyplot as plt\n",
    "from wsi_inference.patch_generation import ImageReader\n",
    "\n",
    "# def one_whole_slide(wsi_name):\n",
    "image_reader = ImageReader(WSI, tile_size=1024, scale_factor=2)\n",
    "mask_details = image_reader.get_mask(magnification=10)\n",
    "\n",
    "coords_list = mask_details[\"list_indices\"]\n",
    "stitch_shape = mask_details[\"shape\"]\n",
    "steps = mask_details[\"step_size\"]\n",
    "scale = mask_details[\"scaling\"]\n",
    "\n",
    "# infer\n",
    "valid_dataset = HubDataset(reader=image_reader,coords_list=coords_list)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
    "\n",
    "model = HubmapModel().cuda()\n",
    "model.load_state_dict(torch.load(model_paths[0]))\n",
    "masks_output = infer(model=model,valid_loader=valid_loader,device=config.DEVICE)\n",
    "\n",
    "# batched_coords = [coords_list[i:i + batch_size] for i in range(0, len(coords_list), batch_size)]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_stitiched(image_list, coords_list, mask_shape, step_size, scaling):\n",
    "\n",
    "#         empty_mask = np.zeros(mask_shape)\n",
    "\n",
    "#         for batch in range(len(image_list)):\n",
    "#             for indx,coords in enumerate(coords_list[batch]):\n",
    "#                 patch = transform.resize(image=image_list[batch][indx],output_shape=(step_size,step_size),mode=\"constant\")\n",
    "#                 empty_mask[int(coords[0]/scaling):int(coords[0]/scaling) + step_size,int(coords[1]/scaling):int(coords[1]/scaling) + step_size] =  patch #np.ones((step_size,step_size))\n",
    "\n",
    "\n",
    "#         return empty_mask\n",
    "\n",
    "def get_stitiched(image_list, coords_list, mask_shape, step_size, scaling):\n",
    "\n",
    "        empty_mask = np.zeros(mask_shape)\n",
    "        for indx,coords in enumerate(coords_list):\n",
    "            patch = transform.resize(image=image_list[indx],output_shape=(step_size,step_size),mode=\"constant\")\n",
    "            empty_mask[int(coords[0]/scaling):int(coords[0]/scaling) + step_size,int(coords[1]/scaling):int(coords[1]/scaling) + step_size] =  patch #np.ones((step_size,step_size))\n",
    "\n",
    "        return empty_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [255, 255, 255, ...,   0,   0,   0],\n",
       "       [255, 255, 255, ...,   0,   0,   0],\n",
       "       [255, 255, 255, ...,   0,   0,   0]], dtype=int16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wsi = get_stitiched(image_list=masks_output,\n",
    "                                     coords_list=coords_list,\n",
    "                                     mask_shape=stitch_shape,\n",
    "                                     step_size=steps,\n",
    "                                     scaling=scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow(out_wsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m io\u001b[39m.\u001b[39mimshow(out_wsi)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "io.imshow(out_wsi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
